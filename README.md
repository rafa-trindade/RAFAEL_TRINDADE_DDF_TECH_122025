# RAFAEL TRINDADE - DDF_TECH_122025

## ğŸ“Œ IntroduÃ§Ã£o
Este repositÃ³rio contÃ©m a resoluÃ§Ã£o do Case TÃ©cnico para a posiÃ§Ã£o Engenheiro de Dados JÃºnior na **Dadosfera**. O projeto foca em uma empresa de E-commerce, utilizando o dataset brasileiro da Olist (Kaggle) para construir uma plataforma de dados ponta a ponta, integrando engenharia moderna, modelagem dimensional e visualiaÃ§Ã£o de dados.

---

## ğŸ› ï¸ Arquitetura Geral da SoluÃ§Ã£o
A arquitetura proposta segue padrÃµes modernos de **Lakehouse** + **Data Warehouse AnalÃ­tico**, combinando **MinIO**, **DuckDB**, **PostgreSQL**, **dbt**, **Pandera** e **Dadosfera**.

### ğŸ“Œ Arquitetura Proposta:

![Arquitetura](docs/data_architecture/arquitetura_proposta.png)

### Principais componentes:

- **Fonte de Dados:** Kaggle - Olist (CSV)
- **Data Lake:** MinIO camada Landing (extra: Bronze, Silver, Gold) 
- **Engine de Processamento:** DuckDB
- **Data Warehouse:** PostgreSQL (Docker)
- **TransformaÃ§Ãµes:** dbt
- **Qualidade de Dados:** Pandera + dbt tests
- **Analytics & BI:** Dadosfera + Metabase
- **Data Apps:** Streamlit

---

## ğŸ“š Mapeamento da DocumentaÃ§Ã£o

### ğŸ—ï¸ Data Architecture
ğŸ“ `docs/data_architecture/`

Descreve a arquitetura tÃ©cnica do projeto em execuÃ§Ã£o:
- Componentes da stack (MinIO, PostgreSQL, DuckDB, Pandera, Docker)
- PapÃ©is e responsabilidades de cada serviÃ§o
- IntegraÃ§Ã£o entre ingestÃ£o, processamento e armazenamento

---

### ğŸ›ï¸ Data Governance
ğŸ“ `docs/data_governance/`

Centraliza as polÃ­ticas e diretrizes do projeto e mapeia como a soluÃ§Ã£o atende,
na prÃ¡tica, aos pilares de **Data Governance**.
- PolÃ­tica de retenÃ§Ã£o baseada em execuÃ§Ãµes tÃ©cnicas (`run_id`)
- DefiniÃ§Ã£o de contratos gerais de qualidade de dados
- EstratÃ©gias seguras de reprocessamento e rollback
- Suporte nativo Ã  auditoria, observabilidade e controle de custos
- GovernanÃ§a aplicada via cÃ³digo e automaÃ§Ã£o

---

### ğŸ§¬ Data Lineage
ğŸ“ `docs/data_lineage/`

Documenta a rastreabilidade ponta a ponta dos dados:
- Origem dos dados
- TransformaÃ§Ãµes por camada (Landing â†’ Raw â†’ Staging â†’ Core â†’ Marts â†’ Dadosfera)

---

### ğŸ§± Data Modeling
ğŸ“ `docs/data_modeling/`

- Documenta as decisÃµes de modelagem de dados adotadas no projeto:
- Modelagem OLTP dos dados de origem
- Modelagem OLAP orientada a analytics
- Diagramas e imagens das estruturas de dados

---

### ğŸ” Data Observability
ğŸ“ `docs/data_observability/`

Mapeia como o projeto atende aos pilares de Data Observability:
- Freshness
- Volume
- Schema
- Distribution
- Lineage
- Quality
- Reliability e Reprocessamento

A observabilidade emerge como resultado das decisÃµes de arquitetura e governanÃ§a.

---

### ğŸ“Š Data Profiling
ğŸ“ `docs/data_profiling/`

Apresenta anÃ¡lises exploratÃ³rias e estatÃ­sticas dos dados:
- Volume por camada
- Cardinalidade
- DistribuiÃ§Ã£o de valores
- Percentual de nulos

Utilizado como base para qualidade e observabilidade.

---

### âš™ï¸ ConfiguraÃ§Ãµes de Infraestrutura
ğŸ“ `docs/configuracoes/`

- Centraliza guias tÃ©cnicos de configuraÃ§Ã£o do ambiente de infraestrutura e serviÃ§os
utilizados no projeto:
- ConfiguraÃ§Ã£o do PostgreSQL em Docker com SSL/TLS habilitado
- CriaÃ§Ã£o, permissÃµes e montagem segura de certificados SSL
- Suporte a acesso seguro por ferramentas externas (ex: Dadosfera)

---

# ğŸ“‘ Itens do Case

## Item 0 - Agilidade e Planejamento

### Metodologia

O planejamento do projeto foi realizado seguindo boas prÃ¡ticas do PMBOK, combinado com metodologias Ã¡geis.

**GestÃ£o:** Quadro Kanban para controle de tarefas, entregas tÃ©cnicas e milestones do projeto.

![Quadro GitHub Projects](docs/images/project.png)


## Item 1 - Base de Dados
**Dataset:** Brazilian E-Commerce Dataset by Olist (Kaggle).

**Justificativa:** 
- Dataset real, amplamente utilizado em projetos analÃ­ticos
- DomÃ­nio aderente ao cenÃ¡rio de e-commerce
- Volume superior a 100.000 registros
- ContÃ©m dados transacionais e descritivos

**Principais tabelas:**
- `olist_orders_dataset`
- `olist_order_items_dataset`
- `olist_products_dataset`
- `olist_customers_dataset`
- `olist_geolocation_dataset`


## Item 2 e 3 - Integrar e Explorar (Dadosfera)

### EstratÃ©gia de IngestÃ£o

A ingestÃ£o foi dividida em etapas claras:

**ExtraÃ§Ã£o Kaggle â†’ MinIO (Landing)**

- Scripts em Python
- Versionamento por `run_id`
- Dados armazenados em formato parquet
- VerificaÃ§Ã£o de Qualidade com Pandera

**Carga AnalÃ­tica no Data Warehouse**

- PostgreSQL utilizado como Data Warehouse analÃ­tico
- TransformaÃ§Ãµes realizadas com dbt
- ConstruÃ§Ã£o do Star Schema (Kimball) diretamente no DW
- AplicaÃ§Ã£o de testes de qualidade (dbt tests)
- MicrotransformaÃ§Ãµes simuladas no contexto analÃ­tico
- Motor de processamento na camada de ingestÃ£o: DuckDB

**Lakehouse: Landing â†’ Bronze / Silver / Gold (Arquitetura BÃ´nus)**

- OrganizaÃ§Ã£o incremental
- PadronizaÃ§Ã£o de schemas
- PreparaÃ§Ã£o para consumo por modelos de ML

**Carga e catalogaÃ§Ã£o dos dados utilizando o mÃ³dulo de Coleta da Dadosfera.**

- MÃ³dulo de Coleta da Dadosfera
- ExecuÃ§Ã£o a partir de VPS dedicada
- PostgreSQL em container com SSL habilitado

A **carga** foi realizada a partir de uma VPS dedicada, configurada para permitir integraÃ§Ã£o segura com a plataforma.

A **catalogaÃ§Ã£o dos dados** foi realizada diretamente na plataforma Dadosfera, onde os ativos ingeridos foram registrados, descritos e organizados, possibilitando sua exploraÃ§Ã£o, governanÃ§a e reutilizaÃ§Ã£o.

**Ativo na Dadosfera:** [[PIPELINE](https://app.dadosfera.ai/pt-BR/collect/pipelines/fb3dc75a-11f8-4c61-99c4-e804871d166d)]  [[LINK PARA O DATASET CATALOGADO](https://app.dadosfera.ai/pt-BR/catalog/data-assets?pipeline_id=fb3dc75a-11f8-4c61-99c4-e804871d166d&pipeline_name=RAFAEL%20TRINDADE%20-%20DDF_TECH_122025)]


## Item 4 â€“ Data Quality

### Abordagem

A qualidade dos dados foi tratada desde o inÃ­cio do pipeline.

### Ferramentas Utilizadas

- **Pandera (Python)** â€“ validaÃ§Ã£o de schemas
- **dbt tests** â€“ testes analÃ­ticos

### EntregÃ¡vel

GeraÃ§Ã£o de relatÃ³rio de qualidade para identificaÃ§Ã£o de nulos e tipos incorretos.

**Resultado:** [[PANDERA REPORTS](https://github.com/rafa-trindade/RAFAEL_TRINDADE_DDF_TECH_122025/tree/main/reports/pandera)]  [[DBT REPORTS](https://github.com/rafa-trindade/RAFAEL_TRINDADE_DDF_TECH_122025/tree/main/reports/dbt)]


## Item 6 â€“ Modelagem de Dados

Modelagem dimensional seguindo os princÃ­pios de Ralph Kimball.

**Esquema:** Star Schema (Tabelas Fato e DimensÃ£o).<br>
**Justificativa:** OtimizaÃ§Ã£o para consultas analÃ­ticas e performance no BI.

### Estrutura Final

**Fato:**

- `fact_order_items`

**DimensÃµes:**

- `dim_customers`
- `dim_products`
- `dim_geolocation`
- `dim_date` *(dbt_seed)*
- `dim_time` *(dbt_seed)*

### `modelo_olap`

![Modelagem](docs/data_modeling/olap.png)

### origem: `modelo_oltp`

![oltp](docs/data_modeling/oltp.png)

---

## Item 7 - Analisar (VisualizaÃ§Ã£o)
Dashboard interativo construÃ­do na Dadosfera (Metabase).
* **AnÃ¡lises:** 
* **Query SQL:** [LINK PARA O ARQUIVO SQL DE CONSULTA]

---

## Item 8 - Pipelines

Pipeline de processamento automatizado utilizando os Steps da Dadosfera.

**Status:** [[PIPELINE](https://app.dadosfera.ai/pt-BR/collect/pipelines/fb3dc75a-11f8-4c61-99c4-e804871d166d)]

---

### Item 9 - Data App (Streamlit)
Desenvolvimento de uma aplicaÃ§Ã£o para exploraÃ§Ã£o de insights.
* **URL do App:** [LINK PARA O STREAMLIT CLOUD]
* **Funcionalidade:**

---

## ğŸ¥ Item 10 - ApresentaÃ§Ã£o (Pitch TÃ©cnico)
ApresentaÃ§Ã£o da soluÃ§Ã£o e demonstraÃ§Ã£o da viabilidade de substituiÃ§Ã£o da arquitetura atual pela Dadosfera.

ğŸ‘‰ **[LINK PARA O VÃDEO NO YOUTUBE - NÃƒO LISTADO]**

---

**PortfÃ³lio:** [https://rafa-trindade.github.io/](https://rafa-trindade.github.io/)<br>
**LinkedIn:** [https://www.linkedin.com/in/rafatrindade/](https://www.linkedin.com/in/rafatrindade/)