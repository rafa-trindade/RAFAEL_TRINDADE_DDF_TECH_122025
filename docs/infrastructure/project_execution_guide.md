## ğŸš€ Guia de ExecuÃ§Ã£o do Projeto

Este guia descreve o passo a passo para executar o projeto, desde a configuraÃ§Ã£o do ambiente atÃ© a visualizaÃ§Ã£o da aplicaÃ§Ã£o analÃ­tica.

---

### ğŸ“¦ PrÃ©-requisitos

- Linux / macOS
- Python Utilizado 3.12.3
- Docker e Docker Compose
- Git (para clonar o repositÃ³rio)
- Acesso Ã  internet (download do dataset Kaggle)

---

### ğŸ” 1. ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente

Crie o arquivo `.env` a partir do template `.env.example`:

```bash
cp .env.example .env
```
Edite o arquivo `.env` e configure as variÃ¡veis de ambiente necessÃ¡rias, incluindo o endpoint e as credenciais do MinIO (S3), as configuraÃ§Ãµes de conexÃ£o do PostgreSQL (Docker) e as credenciais da API do Kaggle.

---

### ğŸ˜ 2. Subir o PostgreSQL (Data Warehouse)

O PostgreSQL Ã© executado via Docker Compose:

```bash
docker compose up -d
```

âš ï¸ Certifique-se de que o PostgreSQL esteja em execuÃ§Ã£o antes de iniciar as etapas de ingestÃ£o de dados.

---

### ğŸ 3. Instalar DependÃªncias Python

#### ğŸ§ª 3.1 CriaÃ§Ã£o e ativaÃ§Ã£o do ambiente virtual

```bash
python -m venv .venv
source .venv/bin/activate
```

---

#### ğŸ“¦ 3.2 InstalaÃ§Ã£o das dependÃªncias do projeto

```bash
pip install -r requirements.txt
```

---

### â˜ï¸ 4. IngestÃ£o de Dados - Kaggle â†’ Data Lake (MinIO)

Executa o pipeline de ingestÃ£o do dataset Olist, convertendo CSV para Parquet e carregando no Data Lake (camada Landing).

```bash
python -m scripts.ingestion.kaggle_to_bucket
```

---

### ğŸ—ï¸ 5. IngestÃ£o de Dados - Data Lake â†’ Data Warehouse

Executa a carga da camada Landing (MinIO) para a camada Raw no PostgreSQL, utilizando DuckDB como engine de processamento.

```bash
python -m scripts.ingestion.bucket_to_dw
```

---

### ğŸ”§ 6. TransformaÃ§Ãµes com dbt

#### 6.1 Garantir permissÃ£o de execuÃ§Ã£o nos scripts

Antes da execuÃ§Ã£o, Ã© necessÃ¡rio garantir permissÃ£o de execuÃ§Ã£o nos scripts:


```bash
chmod +x scripts/dbt/run_staging.sh
chmod +x scripts/dbt/run_core.sh
chmod +x scripts/dbt/run_marts.sh
```

---

#### 6.2 Instalar dependÃªncias do dbt

O projeto utiliza pacotes externos definidos no arquivo `packages.yml`,
como o `dbt_utils`.

Para instalar as dependÃªncias:

```bash
cd dbt
dbt deps
cd ..
```

Esse comando deve ser executado:

- Na primeira execuÃ§Ã£o do projeto
- ApÃ³s atualizaÃ§Ã£o de dependÃªncias
- Em novos ambientes

---

#### 6.3 Criar seeds do dbt

Execute o script responsÃ¡vel por criar e carregar os seeds do dbt no diretÃ³rio `dbt/seeds`:

```bash
python -m scripts.utils.dbt_seeds
```

âš ï¸ Importante: os seeds devem ser criados antes da execuÃ§Ã£o das transformaÃ§Ãµes (staging, core e marts).

---

#### 6.4 ExecuÃ§Ã£o dos Pipelines

#### 1ï¸âƒ£ Staging

```bash
./scripts/dbt/run_staging.sh
```

---

#### 2ï¸âƒ£ Core

```bash
./scripts/dbt/run_core.sh
```

---

#### 3ï¸âƒ£ Marts

```bash
./scripts/dbt/run_marts.sh
```

---

### ğŸ›ï¸ 7. GovernanÃ§a e Qualidade de Dados

#### ğŸ“š Gerar catÃ¡logo de dados (camada Marts)

```bash
python -m scripts.catalog.generate_data_catalog
```

#### ğŸ” Profiling dos dados (Landing - MinIO)

```bash
python -m scripts.profiling.landing.customers
python -m scripts.profiling.landing.geolocation
python -m scripts.profiling.landing.order_items
python -m scripts.profiling.landing.orders
python -m scripts.profiling.landing.products
```

---


### ğŸ“Š 8. AplicaÃ§Ã£o AnalÃ­tica (Streamlit)

#### â–¶ï¸ Subir a aplicaÃ§Ã£o

```bash
./scripts/streamlit/run_streamlit.sh
```

A aplicaÃ§Ã£o pode ser acessada via:

- http://<IP_DO_SERVIDOR>:8501
- http://localhost:8501

#### â¹ï¸ Parar a aplicaÃ§Ã£o

```bash
./scripts/streamlit/stop_streamlit.sh
```

### âš ï¸ ObservaÃ§Ãµes

> Fluxo resumido do pipeline:
> Kaggle â†’ MinIO (Landing) â†’ PostgreSQL (Raw) â†’ dbt (Staging / Core / Marts) â†’ Streamlit

- A execuÃ§Ã£o do projeto Ã© manual, adequada ao contexto de Prova de Conceito (PoC)
- A arquitetura foi projetada para permitir evoluÃ§Ã£o futura com orquestraÃ§Ã£o via Airflow e execuÃ§Ã£o agendada
