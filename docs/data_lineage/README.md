# üîó Data Lineage - Dataset Olist (Clientes e Pedidos)

Este documento descreve o **data lineage end-to-end** da Prova de Conceito (PoC), detalhando a origem dos dados, os processos de ingest√£o, carga e transforma√ß√£o, bem como a evolu√ß√£o dos dados ao longo das camadas do **Data Lake (Landing)** e do **Data Warehouse Anal√≠tico**.

## üì• Ingest√£o - `olist/landing_*`

- **Fonte:** Externa (Kaggle - Dataset Olist)
- **Frequ√™ncia:** Sob demanda (execu√ß√£o manual)
- **Formato de Origem:** CSV
- **Formato Persistido:** Parquet
- **Ambiente de Execu√ß√£o:** VPS
- **Ferramentas:** Python, Kaggle API, Pandera, MinIO

## ‚úÖ Data Lineage - Olist

## 1. Vis√£o Geral

| Item        | Valor |
|------------|-------|
| Origem     | Kaggle - Dataset Olist |
| Dom√≠nio    | Orders / Customers / Products |
| Lake       | Landing (MinIO) |
| Warehouse  | Raw ‚Üí Staging ‚Üí Core ‚Üí Marts |
| Execu√ß√£o   | VPS (ambiente containerizado) |


## 2. Lineage por Camada

### 2.1 Fonte ‚Üí Landing (Data Lake)

**Origem:** Kaggle (CSV)  
**Destino:** `olist/landing_*` (Parquet no MinIO)

kaggle/olist_.csv
‚Üí valida√ß√£o de schema (Pandera)
‚Üí convers√£o para Parquet
‚Üí olist/landing_*


| Etapa | Processo | Descri√ß√£o | A√ß√µes / Regras | Resultado Esperado |
|------:|----------|-----------|----------------|-------------------|
| 1 | Extra√ß√£o | Download dos datasets Olist via Kaggle API | Autentica√ß√£o e versionamento manual | Arquivos CSV dispon√≠veis localmente |
| 2 | Valida√ß√£o de Schema | Valida√ß√£o t√©cnica antes da persist√™ncia | Valida√ß√£o de schema | Dados conformes √† especifica√ß√£o |
| 3 | Convers√£o de Formato | Padroniza√ß√£o para formato anal√≠tico | CSV ‚Üí Parquet | Arquivos otimizados para leitura |
| 4 | Persist√™ncia | Escrita no Data Lake | Upload no MinIO (Landing) | Dados dispon√≠veis no Lake |

üìå **Observa√ß√£o:**  
Nesta etapa **n√£o s√£o aplicadas regras de neg√≥cio**, apenas valida√ß√µes t√©cnicas com **Pandera**.

---

### 2.2 Landing ‚Üí Raw (Data Warehouse)

**Origem:** `olist/landing_*`  
**Destino:** `dw_raw.*` 
**Materializa√ß√£o:** `Table` 


olist/landing_*
‚Üí leitura Parquet
‚Üí carga com DuckDB
‚Üí dw_raw.*


| Etapa | Processo | Descri√ß√£o | A√ß√µes / Regras | Resultado Esperado |
|------:|----------|-----------|----------------|-------------------|
| 1 | Leitura | Leitura dos arquivos Parquet no MinIO | Acesso S3-compatible | Dados dispon√≠veis para carga |
| 2 | Carga | Transfer√™ncia para o DW | DuckDB como engine de carga | Tabelas Raw no PostgreSQL |
| 3 | Persist√™ncia | Armazenamento bruto no DW | Estrutura espelhada da origem | Hist√≥rico fiel √† fonte |

üìå **Observa√ß√£o:**  
A camada **Raw no DW** mant√©m os dados **sem transforma√ß√µes sem√¢nticas**, preservando a estrutura original.

---

### 2.3 Raw ‚Üí Staging (Data Warehouse)

**Origem:** `dw_raw.*`  
**Destino:** `dw_staging.stg_*`  
**Materializa√ß√£o:** `View` 

dw_raw.*
‚Üí padroniza√ß√£o
‚Üí limpeza leve
‚Üí dw_staging.stg_*


| Etapa | Processo | Descri√ß√£o | A√ß√µes / Regras | Resultado Esperado |
|------:|----------|-----------|----------------|-------------------|
| 1 | Padroniza√ß√£o | Ajustes t√©cnicos | Naming convention, tipos, datas | Dados consistentes |
| 2 | Limpeza | Tratamento de valores inv√°lidos | Nulls, formatos, duplicidades t√©cnicas | Base est√°vel para modelagem |
| 3 | Testes | Valida√ß√£o t√©cnica | dbt tests (not null, unique) | Qualidade garantida |

---

### 2.4 Staging ‚Üí Core (Star Schema - Kimball)

**Origem:** `dw_staging.stg_*`  
**Destino:** `dw_core.dim/fact_*`  
**Materializa√ß√£o:** `Table` 

dw_staging.stg_*
‚Üí modelagem dimensional
‚Üí fatos e dimens√µes
‚Üí dw_core.dim/fact_*


| Etapa | Processo | Descri√ß√£o | A√ß√µes / Regras | Resultado Esperado |
|------:|----------|-----------|----------------|-------------------|
| 1 | Modelagem Dimensional | Aplica√ß√£o do Star Schema (Kimball) | Cria√ß√£o de tabelas fato e dimens√µes | Modelo anal√≠tico consistente |
| 2 | Chaves | Defini√ß√£o de relacionamentos | Chaves naturais e t√©cnicas | Integridade dimensional |
| 3 | Regras de Neg√≥cio | Aplica√ß√£o de sem√¢ntica | Status, m√©tricas, filtros | Dados alinhados ao neg√≥cio |

üìå **Observa√ß√£o:**  
A camada **Core** representa a **verdade anal√≠tica** do neg√≥cio.

---

### 2.5 Core ‚Üí Marts (Consumo)

**Origem:** `dw_core.dim/fact_*`  
**Destino:** `dw_marts.mart_*`  
**Materializa√ß√£o:** `Table`

dw_core.dim/fact_*
‚Üí agrega√ß√µes
‚Üí m√©tricas
‚Üí dw_marts.mart_*


| Etapa | Processo | Descri√ß√£o | A√ß√µes / Regras | Resultado Esperado |
|------:|----------|-----------|----------------|-------------------|
| 1 | Agrega√ß√µes | Simplifica√ß√£o para consumo | M√©tricas e KPIs | Consultas perform√°ticas |
| 2 | Especializa√ß√£o | Marts por dom√≠nio | customers, products, sales | Consumo otimizado |
| 3 | Testes | Valida√ß√£o final | dbt tests | Confiabilidade anal√≠tica |


## 3. Consumo e Governan√ßa

- **Metabase**
  - Dashboards anal√≠ticos
  - Integrado √† **Dadosfera**
  - Pipeline e ativos **catalogados**

- **Streamlit**
  - Aplica√ß√µes anal√≠ticas interativas
  - Consumo direto do **Data Warehouse (PostgreSQL)**


## 4. Observa√ß√µes Finais

- O Data Lake possui **apenas a camada Landing**
- **N√£o h√° Bronze / Silver no Lake** (arquitetura de refer√™ncia)
- Todas as transforma√ß√µes sem√¢nticas ocorrem no **Data Warehouse**
- DuckDB atua **exclusivamente como engine de carga**
- A arquitetura segue boas pr√°ticas de **ELT moderno**
- Execu√ß√£o integral em **VPS**, com containers Docker

