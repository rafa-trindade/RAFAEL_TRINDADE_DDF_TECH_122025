# üèóÔ∏è Data Architecture - Mapeamento de Arquitetura do Projeto

Este documento descreve a **arquitetura t√©cnica da Prova de Conceito (PoC)** em execu√ß√£o, detalhando como os componentes se integram para viabilizar os processos de **ingest√£o, processamento, transforma√ß√£o, valida√ß√£o, armazenamento e consumo anal√≠tico de dados**.

A solu√ß√£o segue padr√µes modernos de **Lakehouse + Data Warehouse Anal√≠tico**, priorizando **simplicidade operacional**, **portabilidade** e **boas pr√°ticas de engenharia de dados**.

---

## üõ†Ô∏è Arquitetura Geral da Solu√ß√£o (PoC)

![Arquitetura Geral](../images/arquitetura_proposta.png)

### Vis√£o Geral

O fluxo de dados inicia-se na ingest√£o de arquivos CSV do **dataset Olist (Kaggle)**, executada em uma **VPS**, onde os dados s√£o armazenados no **Data Lake (MinIO)** na camada *Landing*.  
Os dados passam por etapas de **carga com DuckDB**, modelagem anal√≠tica com **dbt no PostgreSQL**, valida√ß√µes de qualidade e s√£o consumidos por ferramentas de **visualiza√ß√£o, cat√°logo e aplica√ß√µes anal√≠ticas**, todas executadas em ambiente **local containerizado dentro da VPS**.


---

## üß± Componentes da Arquitetura

| Componente | Papel na Arquitetura | Responsabilidades T√©cnicas | Diferencial Estrat√©gico |
|-----------|---------------------|----------------------------|-------------------------|
| **Kaggle API** | **Fonte de Dados** | Download automatizado do dataset Olist em formato CSV por meio de scripts Python. | Fonte p√∫blica realista para valida√ß√£o da arquitetura e dos pipelines. |
| **Python (Ingest√£o)** | **Camada de Ingest√£o** | Extra√ß√£o dos dados via Kaggle API, leitura de arquivos CSV, convers√£o para formato Parquet e armazenamento no MinIO. | Flexibilidade para tratamento inicial dos dados e f√°cil integra√ß√£o com bibliotecas anal√≠ticas. |
| **MinIO (S3-compatible)** | **Data Lake (Landing / Bronze / Silver / Gold)** | Armazenamento de dados brutos e processados em formato Parquet na camada landing. | Compatibilidade com S3 API, permitindo migra√ß√£o futura para AWS ou outros clouds sem refatora√ß√£o. |
| **DuckDB** | **Engine de Processamento Anal√≠tico** | Leitura de arquivos Parquet no MinIO, execu√ß√£o de transforma√ß√µes SQL vetorizadas e carga dos dados transformados no Data Warehouse PostgreSQL. | Alta performance anal√≠tica local para transforma√ß√£o de dados, sem depend√™ncia de clusters distribu√≠dos. |
| **dbt (Core + Postgres)** | **Transforma√ß√µes e Modelagem Anal√≠tica** | Cria√ß√£o de modelos anal√≠ticos no PostgreSQL, testes de integridade, documenta√ß√£o e versionamento l√≥gico do warehouse. | Padroniza√ß√£o de transforma√ß√µes e governan√ßa leve, alinhada a boas pr√°ticas modernas. |
| **Pandera** | **Qualidade e Valida√ß√£o de Dados** | Valida√ß√£o de schema **antes da persist√™ncia dos dados na camada Landing do MinIO**, garantindo conformidade na ingest√£o. | Detec√ß√£o precoce de inconsist√™ncias e garantia de contratos de dados desde a origem. |
| **PostgreSQL** | **Data Warehouse Anal√≠tico** | Persist√™ncia de dados modelados para consumo por BI e aplica√ß√µes anal√≠ticas. | Banco relacional robusto, amplamente adotado e integrado ao ecossistema dbt/BI. |
| **Docker** | **Infraestrutura e Isolamento** | Containeriza√ß√£o de servi√ßos (PostgreSQL, MinIO, aplica√ß√µes) executados em uma VPS, garantindo reprodutibilidade do ambiente. | Facilidade de setup local, isolamento de servi√ßos e portabilidade para outros ambientes ou cloud. |
| **Dadosfera** | **Cat√°logo de Dados e Governan√ßa** | Exposi√ß√£o de metadados, documenta√ß√£o e explora√ß√£o dos datasets anal√≠ticos. | Visibilidade, governan√ßa e descoberta de dados em ambiente anal√≠tico. |
| **Metabase** | **Visualiza√ß√£o Anal√≠tica (BI)** | Cria√ß√£o de dashboards e an√°lises explorat√≥rias integrado √† plataforma Dadosfera e utilizando a pipeline local como fonte de dados. | Ferramenta integrada √† camada de governan√ßa e visualiza√ß√£o da Dadosfera para r√°pida valida√ß√£o anal√≠tica em PoCs. |
| **Streamlit** | **Data App Anal√≠tico** | Desenvolvimento de aplica√ß√µes interativas para explora√ß√£o e visualiza√ß√£o de dados a partir do Data Warehouse PostgreSQL. | Agilidade na cria√ß√£o de interfaces anal√≠ticas diretamente conectadas ao DW, sem necessidade de front-end complexo. |


---

## üîÑ Fluxo de Dados (End-to-End)

1. **Ingest√£o**
   - Download dos dados via **Kaggle API** por meio de scripts **Python**
   - Valida√ß√£o de schema, tipos e regras de neg√≥cio com **Pandera**
   - Convers√£o dos arquivos CSV para **Parquet**
   - Armazenamento dos dados validados no **MinIO - Camada Landing**

2. **Carga para o Data Warehouse**
   - Leitura dos arquivos **Parquet** armazenados na camada Landing do **MinIO**
   - Utiliza√ß√£o do **DuckDB exclusivamente como engine de carga**
   - Transfer√™ncia dos dados do Data Lake para a camada **Raw** do **Data Warehouse PostgreSQL**

3. **Modelagem Anal√≠tica no Data Warehouse**
   - Transforma√ß√µes de dados realizadas integralmente no **PostgreSQL** utilizando **dbt**
   - Organiza√ß√£o dos dados nas camadas **Raw**, **Staging**, **Core** e **Marts**, sendo que na camada **Core** √© aplicada a modelagem dimensional em **Star Schema (Kimball)**
   - Execu√ß√£o de **dbt tests** para valida√ß√£o de integridade, consist√™ncia e regras de neg√≥cio

4. **Consumo Anal√≠tico**
   - Explora√ß√£o e visualiza√ß√£o de dados por meio do **Metabase**, integrado √† **Dadosfera**, com **cataloga√ß√£o da pipeline e dos ativos de dados** dentro da plataforma
   - Desenvolvimento de **aplica√ß√µes anal√≠ticas interativas com Streamlit**, consumindo dados diretamente do **Data Warehouse PostgreSQL**


---

## üì¶ Stack Tecnol√≥gica (Requirements)

- **Ingest√£o e Prepara√ß√£o de Dados:** kaggle, pandas, pyarrow, python-dotenv  
- **Qualidade de Dados (Pr√©-Landing):** pandera  
- **Data Lake (S3-compatible):** MinIO, boto3  
- **Engine de Carga para o DW:** duckdb  
- **Data Warehouse:** PostgreSQL, psycopg2-binary, SQLAlchemy  
- **Transforma√ß√µes e Modelagem Anal√≠tica:** dbt-core, dbt-postgres  
- **Visualiza√ß√£o e Data Apps:** streamlit, plotly, tabulate  
- **Governan√ßa e Cat√°logo:** Dadosfera, Metabase  
- **Infraestrutura:** Docker  

---

## üéØ Considera√ß√µes Finais

Esta arquitetura foi projetada para **validar conceitos**, **experimentar boas pr√°ticas modernas de dados** e **demonstrar viabilidade t√©cnica**, sendo executada integralmente em uma **VPS**, com baixo custo operacional e alta flexibilidade para evolu√ß√£o futura.

A solu√ß√£o pode ser facilmente expandida para:
- Orquestra√ß√£o dos pipelines com **Apache Airflow**
- Migra√ß√£o da infraestrutura para **cloud providers** (AWS, GCP ou Azure)
- Migra√ß√£o das **transforma√ß√µes, documenta√ß√£o e governan√ßa de dados** para a plataforma **Dadosfera**
